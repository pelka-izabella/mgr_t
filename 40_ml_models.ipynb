{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'text_prepper'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-dc70d99567a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtext_prepper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrepareText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'text_prepper'"]}],"source":["import os\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from text_prepper import PrepareText\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectKBest\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_confusion_matrix(y_true, y_pred):\n","    cf_matrix = confusion_matrix(y_true, y_pred)\n","    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","    group_counts = [\"{0:0.0f}\".format(value) for value in\n","                    cf_matrix.flatten()]\n","\n","    group_percentages = [\"{0:.2%}\".format(value) for value in\n","                        cf_matrix.flatten()/np.sum(cf_matrix)]\n","\n","    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n","            zip(group_names,group_counts,group_percentages)]\n","    labels = np.asarray(labels).reshape(2,2)\n","\n","    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Setup\n","in_dir = 'data'\n","out_dir = 'data'\n","raw_dataset = 'prepped.csv'\n","\n","df = pd.read_csv(os.path.join(in_dir, raw_dataset), index_col='Unnamed: 0', dtype = {'rating':np.int32, 'review':np.object})\n","\n","df['label'] = 0\n","df.loc[(df.rating == 4)|(df.rating == 5), \"label\"] = 1\n","labels = df['label']\n","\n","prep = PrepareText()\n","df = prep.clean_text(df, in_col='review', out_col='clean_text')\n","df = prep.lemmatize(df, in_col='clean_text', out_col='lem', listed=False)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# select features\n","bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n","features = bow.fit_transform(df['lem']).toarray()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GRID=True \n","\n","if GRID:\n","\n","    model = LogisticRegression(random_state=42, n_jobs=-1)\n","    parameters = {'class_weight':('None', 'balanced'), 'C':[1, 2, 5], 'max_iter':[50,100,200]}\n","    # {'C': 1, 'class_weight': 'None', 'max_iter': 50}\n","    # {'C': 1, 'class_weight': 'balanced', 'max_iter': 100}\n","\n","    # model = SVC(random_state=42)\n","    # parameters = {'kernel':['linear', 'rbf', 'sigmoid']}\n","    # {'kernel': 'rbf'}\n","\n","    # model = RandomForestClassifier(random_state=42, n_jobs=-1)\n","    # parameters = {'n_estimators' :[50, 100,200], 'class_weight':('None', 'balanced', 'balanced_subsample')}\n","    # {'class_weight': 'balanced', 'n_estimators': 50}\n","\n","    # model = AdaBoostClassifier(random_state=42)\n","    # parameters = {'learning_rate':[0.9, 0.95, 1]}\n","    # {'learning_rate': 0.9}\n","\n","    # model = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n","    # parameters = {'n_estimators' :[50, 100,200], 'class_weight':('None', 'balanced', 'balanced_subsample')}\n","    # {'class_weight': 'balanced', 'n_estimators': 100}\n","\n","    scores = ['f1'] # 'precision', 'recall'\n","\n","    for score in scores:\n","        print(model)\n","        print(\"# Tuning hyper-parameters for %s\" % score)\n","        print()\n","\n","        clf = RandomizedSearchCV(\n","            model, parameters, scoring='%s_macro' % score\n","        )\n","        clf.fit(X_train, y_train)\n","\n","        print(\"Best parameters set found on development set:\")\n","        print()\n","        print(clf.best_params_)\n","        print()\n","        print(\"Grid scores on development set:\")\n","        print()\n","        means = clf.cv_results_['mean_test_score']\n","        stds = clf.cv_results_['std_test_score']\n","        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","            print(\"%0.3f (+/-%0.03f) for %r\"\n","                % (mean, std * 2, params))\n","        print()\n","\n","        print(\"Detailed classification report:\")\n","        print()\n","        print(\"The model is trained on the full development set.\")\n","        print(\"The scores are computed on the full evaluation set.\")\n","        print()\n","        y_true, y_pred = y_test, clf.predict(X_test)\n","        print(classification_report(y_true, y_pred))\n","        print()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# selected model\n"," \n","estimators = [\n","    ('svc', SVC(kernel='rbf')),  \n","    ('lr', LogisticRegression(random_state=42, n_jobs=-1, C=1, class_weight='balanced', max_iter=100))\n","]\n","\n","final_estimator = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=50, class_weight='balanced')\n","\n","model = StackingClassifier(cv=3, estimators=estimators, final_estimator=final_estimator)\n","\n","# model = LogisticRegression(random_state=42, n_jobs=-1, C=1, class_weight='balanced', max_iter=100)\n","\n","# model = SVC(random_state=42, kernel='rbf')\n","\n","# model = RandomForestClassifier(random_state=42, n_jobs=-1,class_weight='balanced',n_estimators=100)\n","\n","# model = AdaBoostClassifier(random_state=42, learning_rate=0.9)\n","\n","#  model = ExtraTreesClassifier(random_state=42, n_jobs=-1, class_weight='balanced',n_estimators=100)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model.fit(X_train, y_train)\n","y_true, y_pred = y_test, model.predict(X_test)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model, \"\\n\")\n","plot_confusion_matrix(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model, \"\\n\")\n","print(\"Classification report \\n\", classification_report(y_true,y_pred))\n","print(\"Accuracy: \\t\", round(accuracy_score(y_true, y_pred)*100, 4), \"%\")\n","# Precision = TruePositives / (TruePositives + FalsePositives)\n","print(\"Precision: \\t\", round(precision_score(y_true, y_pred)*100, 4), \"%\")\n","# Recall = TruePositives / (TruePositives + FalseNegatives)\n","print(\"Recall: \\t\", round(recall_score(y_true, y_pred)*100, 4), \"%\")\n","# F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n","print(\"F1 score: \\t\", round(f1_score(y_true, y_pred)*100, 4), \"%\")\n","\n","# Precision: Appropriate when minimizing false positives is the focus.\n","# Recall: Appropriate when minimizing false negatives is the focus.\n","# F1-measure, which weights precision and recall equally, is the variant most often used when learning from imbalanced data."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}